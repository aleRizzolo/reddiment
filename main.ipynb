{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import praw\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Upvotes\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy', 'disney',\n",
    "                 'WhitePeopleonTwitter', 'politics', 'Republican', 'worldnews']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "for iteration, submission in enumerate(subreddit.hot(limit=25)):\n",
    "    print(f\"post {iteration}/25\")\n",
    "    # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "    if submission.id not in df[\"Id\"].values:\n",
    "        # Add submission title to the titles list\n",
    "        titles.append(submission.title)\n",
    "        scores.append(submission.score)  # Add upvotes to the scores list\n",
    "        ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "        # Fetch comments for the current submission\n",
    "        submission.comments.replace_more(limit=25)\n",
    "        submission_comments = []\n",
    "        for comment in submission.comments.list():\n",
    "            # Check if the comment author's username contains \"bot\"\n",
    "            if 'bot' not in comment.author.name.lower():\n",
    "                # Use BeautifulSoup to remove HTML tags from content\n",
    "                soup = BeautifulSoup(comment.body, 'html.parser')\n",
    "                filtered_content = soup.get_text()\n",
    "\n",
    "                # Remove URLs from filtered_content\n",
    "                filtered_content = re.sub(\n",
    "                    r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                # Remove only #\n",
    "                filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                submission_comments.append(filtered_content)\n",
    "        comments.append(submission_comments)\n",
    "\n",
    "    # Create a DataFrame with the new data\n",
    "    new_data = pandas.DataFrame(\n",
    "        {\"Title\": titles, \"Id\": ids, \"Upvotes\": scores, \"Comments\": comments}\n",
    "    )\n",
    "\n",
    "    # Append/concat the new data to the existing DataFrame\n",
    "    df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "    # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "    df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "\n",
    "# Clean dataset\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned = df_cleaned.select_dtypes(include=['object']).applymap(\n",
    "    lambda x: re.sub(r'[^a-zA-Z]', ' ', str(x)))\n",
    "df_cleaned = df_cleaned.applymap(\n",
    "    lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df_cleaned = df_cleaned.replace('nan', np.nan).dropna()\n",
    "\n",
    "# Tokenize the text data\n",
    "df_cleaned['tokenized_text'] = df_cleaned['Comments'].apply(\n",
    "    lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords_italian = set(nltk.corpus.stopwords.words(\"italian\"))\n",
    "df_cleaned['tokenized_text'] = df_cleaned['tokenized_text'].apply(lambda tokens: [\n",
    "    token for token in tokens if token.lower() not in stopwords_english and token.lower() not in stopwords_italian])\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates and cleaning the text data.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
