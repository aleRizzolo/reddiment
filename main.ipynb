{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas\n",
    "import praw\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Upvotes\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy',\n",
    "                 'WhitePeopleTwitter', 'politics', 'Republican', 'worldnews']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "    for iteration, submission in enumerate(subreddit.hot(limit=5)):\n",
    "        print(f\"post {iteration}/5\")\n",
    "        # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "        if submission.id not in df[\"Id\"].values:\n",
    "            # Add submission title to the titles list\n",
    "            titles.append(submission.title)\n",
    "            scores.append(submission.score)  # Add upvotes to the scores list\n",
    "            ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "            # Fetch comments for the current submission\n",
    "            submission.comments.replace_more(limit=25)\n",
    "            submission_comments = []\n",
    "            for comment in submission.comments.list():\n",
    "                # Check if the comment author's username contains \"bot\"\n",
    "                if 'bot' not in comment.name:\n",
    "                    # Use BeautifulSoup to remove HTML tags from content\n",
    "                    soup = BeautifulSoup(comment.body, 'lxml')\n",
    "                    filtered_content = soup.get_text()\n",
    "\n",
    "                    # Remove URLs from filtered_content\n",
    "                    filtered_content = re.sub(\n",
    "                        r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                    # Remove only #\n",
    "                    filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                    submission_comments.append(filtered_content)\n",
    "            comments.append(submission_comments)\n",
    "\n",
    "        # Create a DataFrame with the new data\n",
    "        new_data = pandas.DataFrame(\n",
    "            {\"Title\": titles, \"Id\": ids, \"Upvotes\": scores, \"Comments\": comments}\n",
    "        )\n",
    "\n",
    "        # Append/concat the new data to the existing DataFrame\n",
    "        df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "        df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "    # Save the DataFrame to the CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m df_cleaned \u001b[39m=\u001b[39m df_cleaned\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mnan)\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m     22\u001b[0m \u001b[39m# Tokenize the text data\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df_cleaned[\u001b[39m'\u001b[39m\u001b[39mtokenized_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_cleaned[\u001b[39m'\u001b[39;49m\u001b[39mComments\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m     24\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: word_tokenize(\u001b[39mstr\u001b[39;49m(x)))\n\u001b[0;32m     26\u001b[0m \u001b[39m# Removal of stopwords\u001b[39;00m\n\u001b[0;32m     27\u001b[0m stopwords_english \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mstopwords\u001b[39m.\u001b[39mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m df_cleaned \u001b[39m=\u001b[39m df_cleaned\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mnan)\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m     22\u001b[0m \u001b[39m# Tokenize the text data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m df_cleaned[\u001b[39m'\u001b[39m\u001b[39mtokenized_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_cleaned[\u001b[39m'\u001b[39m\u001b[39mComments\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mlambda\u001b[39;00m x: word_tokenize(\u001b[39mstr\u001b[39;49m(x)))\n\u001b[0;32m     26\u001b[0m \u001b[39m# Removal of stopwords\u001b[39;00m\n\u001b[0;32m     27\u001b[0m stopwords_english \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mstopwords\u001b[39m.\u001b[39mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tokenize\\destructive.py:178\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    175\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m text \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m regexp, substitution \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mENDING_QUOTES:\n\u001b[1;32m--> 178\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(substitution, text)\n\u001b[0;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m regexp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m    181\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1 \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2 \u001b[39m\u001b[39m\"\u001b[39m, text)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "csv_tokenized = \"tokenized_csv.csv\"\n",
    "csv_input = \"reddit_posts_with_comments.csv\"\n",
    "df = pd.read_csv(csv_input)\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned = df_cleaned.select_dtypes(include=['object']).applymap(\n",
    "    lambda x: re.sub(r'[^a-zA-Z]', ' ', str(x)))\n",
    "df_cleaned = df_cleaned.applymap(\n",
    "    lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df_cleaned = df_cleaned.replace('nan', np.nan).dropna()\n",
    "\n",
    "# Tokenize the text data\n",
    "df_cleaned['tokenized_text'] = df_cleaned['Comments'].apply(\n",
    "    lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "df_cleaned['tokenized_text'] = df_cleaned['tokenized_text'].apply(lambda tokens: [\n",
    "    token for token in tokens if token.lower() not in stopwords_english ])\n",
    "\n",
    "# Remove tokens with a single character\n",
    "df_cleaned['tokenized_text'] = df_cleaned['tokenized_text'].apply(lambda tokens: [\n",
    "    token for token in tokens if len(token) > 1])\n",
    "\n",
    "# Drop the unnecessary columns (keep only the 'tokenized_text' column)\n",
    "df_cleaned = df_cleaned[['tokenized_text']]\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_tokenized, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_tokenized}' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.linear_model.logistic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(csv_file)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Initialize the Sonar hate speech analyzer\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m sonar \u001b[39m=\u001b[39m Sonar()\n\u001b[0;32m     11\u001b[0m \u001b[39m# List to store the results of hate speech analysis for each comment\u001b[39;00m\n\u001b[0;32m     12\u001b[0m results \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\hatesonar\\api.py:21\u001b[0m, in \u001b[0;36mSonar.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m model_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(BASE_DIR, \u001b[39m'\u001b[39m\u001b[39mmodel.joblib\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m preprocessor_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(BASE_DIR, \u001b[39m'\u001b[39m\u001b[39mpreprocess.joblib\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(model_file)\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessor \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(preprocessor_file)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[0;32m    659\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[0;32m   1214\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pickle.py:1529\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1527\u001b[0m module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1528\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1529\u001b[0m klass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_class(module, name)\n\u001b[0;32m   1530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(klass)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pickle.py:1580\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1578\u001b[0m     \u001b[39melif\u001b[39;00m module \u001b[39min\u001b[39;00m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING:\n\u001b[0;32m   1579\u001b[0m         module \u001b[39m=\u001b[39m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[1;32m-> 1580\u001b[0m \u001b[39m__import__\u001b[39;49m(module, level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m   1581\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m   1582\u001b[0m     \u001b[39mreturn\u001b[39;00m _getattribute(sys\u001b[39m.\u001b[39mmodules[module], name)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.linear_model.logistic'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hatesonar import Sonar\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "csv_file = \"reddit_posts_with_comments.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Initialize the Sonar hate speech analyzer\n",
    "sonar = Sonar()\n",
    "\n",
    "# List to store the results of hate speech analysis for each comment\n",
    "results = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    comments = eval(row[\"Comments\"])  # Assuming the comments column is a string representation of a list, convert it back to a list using eval()\n",
    "    \n",
    "    for comment in comments:\n",
    "        result = sonar.ping(text=comment)\n",
    "        results.append(result)\n",
    "\n",
    "# Optionally, you can add the results to a new DataFrame if you want to analyze them further\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, you can save the results DataFrame to a new CSV file\n",
    "results_csv_file = \"results_hate_speech_analysis.csv\"\n",
    "results_df.to_csv(results_csv_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
