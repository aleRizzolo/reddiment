{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabriele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabriele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file does not exist. It will be created after fetching new data.\n",
      "conspiracy\n",
      "post 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriele\\AppData\\Local\\Temp\\ipykernel_13544\\950554949.py:69: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(comment.body, 'lxml')\n",
      "C:\\Users\\gabriele\\AppData\\Local\\Temp\\ipykernel_13544\\950554949.py:69: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(comment.body, 'lxml')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post 1/5\n",
      "post 2/5\n",
      "post 3/5\n",
      "post 4/5\n",
      "WhitePeopleTwitter\n",
      "post 0/5\n",
      "post 1/5\n",
      "post 2/5\n",
      "post 3/5\n",
      "post 4/5\n",
      "politics\n",
      "post 0/5\n",
      "post 1/5\n",
      "post 2/5\n",
      "post 3/5\n",
      "post 4/5\n",
      "Republican\n",
      "post 0/5\n",
      "post 1/5\n",
      "post 2/5\n",
      "post 3/5\n",
      "post 4/5\n",
      "worldnews\n",
      "post 0/5\n",
      "post 1/5\n",
      "post 2/5\n",
      "post 3/5\n",
      "post 4/5\n",
      "(25, 4)\n",
      "                                               Title       Id Upvotes  \\\n",
      "0                                     What are they?  15l0ipw    1556   \n",
      "1                                        Based Niger  15kmbz8    4527   \n",
      "2                The truth is staring us in the face  15liyrq      38   \n",
      "3  Uh-oh... WEF took a couple of articles down. A...  15l61t8     162   \n",
      "4  Aaaaand, they’re gone! $500 million gone. It's...  15kul4m     616   \n",
      "5                        The Alabama Riverboat brawl  15lelpm   11541   \n",
      "6  It's kinda wild that GOP's main twitter accoun...  15ld1k8    4573   \n",
      "7                          Why do you think that is?  15lfmot    2684   \n",
      "8                               Interesting question  15l3ucd   18210   \n",
      "9                         So close! These are verbs…  15lc85l    1734   \n",
      "\n",
      "                                            Comments  \n",
      "0  [[meta] sticky comment\\n\\n[rule 2]( ***does no...  \n",
      "1  [[meta] sticky comment\\n\\n[rule 2]( ***does no...  \n",
      "2  [[meta] sticky comment\\n\\n[rule 2]( ***does no...  \n",
      "3  [[meta] sticky comment\\n\\n[rule 2]( ***does no...  \n",
      "4  [[meta] sticky comment\\n\\n[rule 2]( ***does no...  \n",
      "5  [one thing missing to make this all perfect is...  \n",
      "6  [gqp is post-facts.  they don't care that it i...  \n",
      "7  [it’s as much of a mystery as to why black peo...  \n",
      "8  [you mean candidates would have to have platfo...  \n",
      "9  [who would've guessed the \"speak english!\" cro...  \n",
      "CSV file 'reddit_posts_with_comments.csv' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas\n",
    "import praw\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Upvotes\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy',\n",
    "                 'WhitePeopleTwitter', 'politics', 'Republican', 'worldnews']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "    for iteration, submission in enumerate(subreddit.hot(limit=5)):\n",
    "        print(f\"post {iteration}/5\")\n",
    "        # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "        if submission.id not in df[\"Id\"].values:\n",
    "            # Add submission title to the titles list\n",
    "            titles.append(submission.title)\n",
    "            scores.append(submission.score)  # Add upvotes to the scores list\n",
    "            ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "            # Fetch comments for the current submission\n",
    "            submission.comments.replace_more(limit=25)\n",
    "            submission_comments = []\n",
    "            for comment in submission.comments.list():\n",
    "                # Check if the comment author's username contains \"bot\"\n",
    "                if 'bot' not in comment.name:\n",
    "                    # Use BeautifulSoup to remove HTML tags from content\n",
    "                    soup = BeautifulSoup(comment.body, 'lxml')\n",
    "                    filtered_content = soup.get_text()\n",
    "\n",
    "                    # Remove URLs from filtered_content\n",
    "                    filtered_content = re.sub(\n",
    "                        r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                    # Remove only #\n",
    "                    filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                    submission_comments.append(filtered_content)\n",
    "            comments.append(submission_comments)\n",
    "\n",
    "        # Create a DataFrame with the new data\n",
    "        new_data = pandas.DataFrame(\n",
    "            {\"Title\": titles, \"Id\": ids, \"Upvotes\": scores, \"Comments\": comments}\n",
    "        )\n",
    "\n",
    "        # Append/concat the new data to the existing DataFrame\n",
    "        df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "        df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "    # Save the DataFrame to the CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1)\n",
      "                                      tokenized_text\n",
      "0  [meta, sticky, comment, rule, apply, replying,...\n",
      "1  [meta, sticky, comment, rule, apply, replying,...\n",
      "2  [meta, sticky, comment, rule, apply, replying,...\n",
      "3  [meta, sticky, comment, rule, apply, replying,...\n",
      "4  [meta, sticky, comment, rule, apply, replying,...\n",
      "5  [one, thing, missing, make, perfect, police, s...\n",
      "6  [gqp, post, facts, care, wrong, amount, time, ...\n",
      "7  [much, mystery, black, people, tend, support, ...\n",
      "8  [mean, candidates, would, platforms, popular, ...\n",
      "9  [would, guessed, speak, english, crowd, know, ...\n",
      "CSV file 'tokenized_csv.csv' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "csv_tokenized = \"tokenized_csv.csv\"\n",
    "csv_input = \"reddit_posts_with_comments.csv\"\n",
    "df = pd.read_csv(csv_input)\n",
    "\n",
    "# Function to clean the text using regex\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Clean the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokenized_text'] = df['Comments'].apply(word_tokenize)\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if token not in stopwords_english])\n",
    "\n",
    "# Remove tokens with a single character\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if len(token) > 1])\n",
    "\n",
    "# Drop the unnecessary columns (keep only the 'tokenized_text' column)\n",
    "df_cleaned = df[['tokenized_text']]\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_tokenized, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_tokenized}' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.linear_model.logistic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(csv_file)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Initialize the Sonar hate speech analyzer\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m sonar \u001b[39m=\u001b[39m Sonar()\n\u001b[0;32m     11\u001b[0m \u001b[39m# Function to perform hate speech analysis on a batch of comments\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalyze_comments_batch\u001b[39m(comments):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hatesonar\\api.py:21\u001b[0m, in \u001b[0;36mSonar.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m model_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(BASE_DIR, \u001b[39m'\u001b[39m\u001b[39mmodel.joblib\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m preprocessor_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(BASE_DIR, \u001b[39m'\u001b[39m\u001b[39mpreprocess.joblib\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(model_file)\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessor \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(preprocessor_file)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[0;32m    659\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[0;32m   1214\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\pickle.py:1529\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1527\u001b[0m module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1528\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1529\u001b[0m klass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_class(module, name)\n\u001b[0;32m   1530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(klass)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\pickle.py:1580\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1578\u001b[0m     \u001b[39melif\u001b[39;00m module \u001b[39min\u001b[39;00m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING:\n\u001b[0;32m   1579\u001b[0m         module \u001b[39m=\u001b[39m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[1;32m-> 1580\u001b[0m \u001b[39m__import__\u001b[39;49m(module, level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m   1581\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m   1582\u001b[0m     \u001b[39mreturn\u001b[39;00m _getattribute(sys\u001b[39m.\u001b[39mmodules[module], name)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.linear_model.logistic'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hatesonar import Sonar\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "csv_file = \"reddit_posts_with_comments.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Initialize the Sonar hate speech analyzer\n",
    "sonar = Sonar()\n",
    "\n",
    "# List to store the results of hate speech analysis for each comment\n",
    "results = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    comments = eval(row[\"Comments\"])  # Assuming the comments column is a string representation of a list, convert it back to a list using eval()\n",
    "    \n",
    "    for comment in comments:\n",
    "        result = sonar.ping(text=comment)\n",
    "        results.append(result)\n",
    "\n",
    "# Optionally, you can add the results to a new DataFrame if you want to analyze them further\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, you can save the results DataFrame to a new CSV file\n",
    "results_csv_file = \"results_hate_speech_analysis.csv\"\n",
    "results_df.to_csv(results_csv_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
