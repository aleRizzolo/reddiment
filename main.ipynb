{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file already exists. Appending new data and avoiding duplicates.\n",
      "conspiracy\n",
      "post 0/5\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 69\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m comment \u001b[39min\u001b[39;00m submission\u001b[39m.\u001b[39mcomments\u001b[39m.\u001b[39mlist():\n\u001b[0;32m     66\u001b[0m     \u001b[39m# Check if the comment author's username contains \"bot\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbot\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m comment\u001b[39m.\u001b[39mname:\n\u001b[0;32m     68\u001b[0m         \u001b[39m# Use BeautifulSoup to remove HTML tags from content\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m         soup \u001b[39m=\u001b[39m BeautifulSoup(comment\u001b[39m.\u001b[39;49mbody, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     70\u001b[0m         filtered_content \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mget_text()\n\u001b[0;32m     72\u001b[0m         \u001b[39m# Remove URLs from filtered_content\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[0;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[0;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "############## COLLECTING REAL DATA AND CREATING CASE STUDY DATASET##################### \n",
    "import pandas\n",
    "import praw\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Upvotes\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy',\n",
    "                 'WhitePeopleTwitter', 'politics', 'Republican', 'worldnews']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "    for iteration, submission in enumerate(subreddit.hot(limit=5)):\n",
    "        print(f\"post {iteration}/5\")\n",
    "        # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "        if submission.id not in df[\"Id\"].values:\n",
    "            # Add submission title to the titles list\n",
    "            titles.append(submission.title)\n",
    "            scores.append(submission.score)  # Add upvotes to the scores list\n",
    "            ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "            # Fetch comments for the current submission\n",
    "            submission.comments.replace_more(limit=25)\n",
    "            submission_comments = []\n",
    "            for comment in submission.comments.list():\n",
    "                # Check if the comment author's username contains \"bot\"\n",
    "                if 'bot' not in comment.name:\n",
    "                    # Use BeautifulSoup to remove HTML tags from content\n",
    "                    soup = BeautifulSoup(comment.body, 'html.parser')\n",
    "                    filtered_content = soup.get_text()\n",
    "\n",
    "                    # Remove URLs from filtered_content\n",
    "                    filtered_content = re.sub(\n",
    "                        r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                    # Remove only #\n",
    "                    filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                    submission_comments.append(filtered_content)\n",
    "            comments.append(submission_comments)\n",
    "\n",
    "        # Create a DataFrame with the new data\n",
    "        new_data = pandas.DataFrame(\n",
    "            {\"Title\": titles, \"Id\": ids, \"Upvotes\": scores, \"Comments\": comments}\n",
    "        )\n",
    "\n",
    "        # Append/concat the new data to the existing DataFrame\n",
    "        df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "        df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "    # Save the DataFrame to the CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############CLEANING THE DATASET ####################\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv = \"reddit_posts_with_comments.csv\"\n",
    "output_csv = \"cleaned_reddit_posts.csv\"\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Function to find and remove emojis from a string\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Function to clean and filter a single comment\n",
    "def clean_and_filter_comment(comment):\n",
    "    # Convert the string representation of the list to an actual list\n",
    "    comment_list = ast.literal_eval(comment)\n",
    "    # Filter out comments that match the specified patterns\n",
    "    cleaned_comments = []\n",
    "    for c in comment_list:\n",
    "        # Remove emojis from the comment\n",
    "        c = remove_emojis(c)\n",
    "        # Remove links\n",
    "        c = re.sub(r'http[s]?://\\S+', '', c)\n",
    "        # Remove the \"[meta]\" pattern (case-insensitive)\n",
    "        if not re.search(r'\\[meta\\]', c, re.IGNORECASE):\n",
    "            # Remove extra spaces and append the cleaned comment\n",
    "            cleaned_comments.append(re.sub(r'\\s+', ' ', c.strip()))\n",
    "    return cleaned_comments\n",
    "\n",
    "# Apply the cleaning and filtering function to the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_and_filter_comment)\n",
    "\n",
    "# Remove rows where all comments were filtered out\n",
    "df = df[df['Comments'].apply(len) > 0]\n",
    "\n",
    "# Save the cleaned and filtered DataFrame to a new CSV file\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TOKENIZING THE TARGET DATASET ##################### \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "csv_tokenized = \"tokenized_csv.csv\"\n",
    "csv_input = \"reddit_posts_with_comments.csv\"\n",
    "df = pd.read_csv(csv_input)\n",
    "\n",
    "# Function to clean the text using regex\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Clean the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokenized_text'] = df['Comments'].apply(word_tokenize)\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if token not in stopwords_english])\n",
    "\n",
    "# Remove tokens with a single character\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if len(token) > 1])\n",
    "\n",
    "# Drop the unnecessary columns (keep only the 'tokenized_text' column)\n",
    "df_cleaned = df[['tokenized_text']]\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_tokenized, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_tokenized}' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### DOWNLOADING AND CACHING MODELS ################################\n",
    "import logging\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define a mapping of languages to model names\n",
    "language_to_model = {\n",
    "    'en': \"IMSyPP/hate_speech_en\",\n",
    "    'it': \"IMSyPP/hate_speech_it\",\n",
    "    'nl': \"IMSyPP/hate_speech_nl\",\n",
    "    'sl': \"IMSyPP/hate_speech_slo\",\n",
    "}\n",
    "\n",
    "# Function to download models to the cache directory\n",
    "def download_models_to_cache():\n",
    "    # Specify the cache directory for local caching\n",
    "    cache_dir = \".cache\"\n",
    "    # Loop over the models and download them to the cache\n",
    "    for model_name in language_to_model.values():\n",
    "        try:\n",
    "            logger.info(f\"Downloading and caching model '{model_name}'...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "            logger.info(f\"Model '{model_name}' downloaded and cached successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while downloading the model '{model_name}': {e}\")\n",
    "\n",
    "# Call the function to download models to the cache\n",
    "download_models_to_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Token None for key bos_token should be a str or an AddedToken instance",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m final_results \u001b[39m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[39m# Load the default model once for cases where language detection fails\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m default_tokenizer, default_model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     77\u001b[0m \u001b[39m# Iterate over rows in the CSV\u001b[39;00m\n\u001b[0;32m     78\u001b[0m progress_bar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39mtotal_comments, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing Comments\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     37\u001b[0m     special_tokens[\u001b[39m\"\u001b[39m\u001b[39meos_token\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39meos_token_id\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m special_tokens:\n\u001b[1;32m---> 40\u001b[0m     tokenizer\u001b[39m.\u001b[39;49madd_special_tokens(special_tokens)\n\u001b[0;32m     42\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(model_name, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer, model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:949\u001b[0m, in \u001b[0;36mSpecialTokensMixin.add_special_tokens\u001b[1;34m(self, special_tokens_dict, replace_additional_special_tokens)\u001b[0m\n\u001b[0;32m    947\u001b[0m     added_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_tokens(value, special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    948\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 949\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    950\u001b[0m         value, (\u001b[39mstr\u001b[39m, AddedToken)\n\u001b[0;32m    951\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mToken \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m for key \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m should be a str or an AddedToken instance\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    952\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, key, value)\n\u001b[0;32m    953\u001b[0m     added_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_tokens([value], special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Token None for key bos_token should be a str or an AddedToken instance"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the Reddit posts CSV file\n",
    "input_csv = \"cleaned_reddit_posts.csv\"\n",
    "output_csv = \"reddit_posts_with_labels.csv\"\n",
    "\n",
    "# Define a mapping of languages to model names\n",
    "language_to_model = {\n",
    "    'en': \"IMSyPP/hate_speech_en\",\n",
    "    'it': \"IMSyPP/hate_speech_it\",\n",
    "    'nl': \"IMSyPP/hate_speech_nl\",\n",
    "    'sl': \"IMSyPP/hate_speech_slo\",\n",
    "}\n",
    "\n",
    "# Define the default model for cases where language detection fails\n",
    "default_model_name = \"IMSyPP/hate_speech_en\"\n",
    "\n",
    "def load_model(language):\n",
    "    model_name = language_to_model.get(language, default_model_name)\n",
    "    cache_dir = \".cache\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    # Set special tokens (if applicable)\n",
    "    special_tokens = {}\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens[\"bos_token\"] = \"[BOS]\"  # Use the string token here\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens[\"eos_token\"] = \"[EOS]\"  # Use the string token here\n",
    "    \n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to analyze a comment and return the results\n",
    "def analyze_comment(comment, language, tokenizer, model):\n",
    "    try:\n",
    "        inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "        analyze_error = None\n",
    "    except Exception as e:\n",
    "        probabilities = [0.0] * 4\n",
    "        analyze_error = str(e)\n",
    "    return probabilities, language, analyze_error\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Total number of comments to process\n",
    "total_comments = df['Comments'].apply(len).sum()\n",
    "\n",
    "# Batch size for writing results to CSV\n",
    "batch_size = 100\n",
    "\n",
    "# Processed comment count\n",
    "processed_comments = 0\n",
    "\n",
    "# Initialize lists to store final results\n",
    "final_results = []\n",
    "\n",
    "# Load the default model once for cases where language detection fails\n",
    "default_tokenizer, default_model = load_model('en')\n",
    "\n",
    "# Iterate over rows in the CSV\n",
    "progress_bar = tqdm(total=total_comments, desc=\"Processing Comments\", leave=False)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    comment_list = eval(row['Comments'])  # Assuming the comments are in a list format\n",
    "\n",
    "    # Split the comment list into batches\n",
    "    comment_batches = [comment_list[i:i + batch_size] for i in range(0, len(comment_list), batch_size)]\n",
    "\n",
    "    for batch in comment_batches:\n",
    "        try:\n",
    "            # Filter out empty or very short comments\n",
    "            batch = [comment for comment in batch if len(comment.strip()) > 10]  # Adjust the length threshold as needed\n",
    "            \n",
    "            if not batch:\n",
    "                continue  # Skip the batch if no valid comments are present\n",
    "            \n",
    "            # Detect language for the first comment in the batch\n",
    "            language = detect(batch[0])\n",
    "            \n",
    "            # Load the model and tokenizer for the detected language\n",
    "            tokenizer, model = load_model(language)\n",
    "            \n",
    "            # Process the batch of comments sequentially\n",
    "            batch_texts = batch  # Store the batch of comments as a list of strings\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs.logits, dim=1).tolist()\n",
    "            analyze_error = None\n",
    "\n",
    "            # Process the results for each comment in the batch\n",
    "            for i, comment in enumerate(batch_texts):\n",
    "                result_row = (comment, probabilities[i][1], probabilities[i][2], probabilities[i][3], language, analyze_error)\n",
    "                final_results.append(result_row)\n",
    "\n",
    "                # Update processed comment count\n",
    "                processed_comments += 1\n",
    "\n",
    "                # Save results to CSV every batch_size comments\n",
    "                if processed_comments % batch_size == 0:\n",
    "                    result_df = pd.DataFrame(final_results, columns=['comment', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "                    result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "                # Update progress description and postfix\n",
    "                progress_percent = (processed_comments / total_comments) * 100\n",
    "                progress_bar.set_description(f\"Processing Comments - {progress_percent:.2f}%\")\n",
    "                progress_bar.set_postfix({\"Processed\": f\"{processed_comments}/{total_comments}\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error processing batch: %s\", str(e))\n",
    "\n",
    "# Save the final results\n",
    "result_df = pd.DataFrame(final_results, columns=['comment', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "logger.info(\"Analysis completed. Results saved to: %s\", output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
