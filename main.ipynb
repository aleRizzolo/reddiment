{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## COLLECTING REAL DATA AND CREATING CASE STUDY DATASET##################### \n",
    "import pandas\n",
    "import praw\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy',\n",
    "                 'WhitePeopleTwitter', 'politics', 'Republican', 'worldnews', 'CombatFootage', 'UkraineRussiaReport']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "    for iteration, submission in enumerate(subreddit.hot(limit=5)):\n",
    "        print(f\"post {iteration}/5\")\n",
    "        # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "        if submission.id not in df[\"Id\"].values:\n",
    "            # Add submission title to the titles list\n",
    "            titles.append(submission.title)\n",
    "            ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "            # Fetch comments for the current submission\n",
    "            submission.comments.replace_more(limit=25)\n",
    "            submission_comments = []\n",
    "            for comment in submission.comments.list():\n",
    "                # Check if the comment author's username contains \"bot\"\n",
    "                if 'bot' not in comment.name:\n",
    "                    # Use BeautifulSoup to remove HTML tags from content\n",
    "                    soup = BeautifulSoup(comment.body, 'html.parser')\n",
    "                    filtered_content = soup.get_text()\n",
    "\n",
    "                    # Remove URLs from filtered_content\n",
    "                    filtered_content = re.sub(\n",
    "                        r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                    # Remove only #\n",
    "                    filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                    submission_comments.append(filtered_content)\n",
    "            comments.append(submission_comments)\n",
    "\n",
    "        # Create a DataFrame with the new data\n",
    "        new_data = pandas.DataFrame(\n",
    "            {\"Title\": titles, \"Id\": ids, \"Comments\": comments}\n",
    "        )\n",
    "\n",
    "        # Append/concat the new data to the existing DataFrame\n",
    "        df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "        df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "    # Save the DataFrame to the CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############CLEANING THE DATASET ####################\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv = \"reddit_posts_with_comments.csv\"\n",
    "output_csv = \"cleaned_reddit_posts.csv\"\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Function to find and remove emojis from a string\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Function to clean and filter a single comment\n",
    "def clean_and_filter_comment(comment):\n",
    "    # Convert the string representation of the list to an actual list\n",
    "    comment_list = ast.literal_eval(comment)\n",
    "    # Filter out comments that match the specified patterns\n",
    "    cleaned_comments = []\n",
    "    for c in comment_list:\n",
    "        # Remove '[deleted]' comments\n",
    "        if c.strip() != \"[deleted]\":\n",
    "            # Remove emojis from the comment\n",
    "            c = remove_emojis(c)\n",
    "            # Remove links\n",
    "            c = re.sub(r'http[s]?://\\S+', '', c)\n",
    "            # Remove the \"[meta]\" pattern (case-insensitive)\n",
    "            if not re.search(r'\\[meta\\]', c, re.IGNORECASE):\n",
    "                # Remove extra spaces and append the cleaned comment\n",
    "                cleaned_comments.append(re.sub(r'\\s+', ' ', c.strip()))\n",
    "    return cleaned_comments\n",
    "\n",
    "# Apply the cleaning and filtering function to the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_and_filter_comment)\n",
    "\n",
    "# Remove rows where all comments were filtered out\n",
    "df = df[df['Comments'].apply(len) > 0]\n",
    "\n",
    "# Save the cleaned and filtered DataFrame to a new CSV file\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TOKENIZING THE TARGET DATASET ##################### \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "csv_tokenized = \"tokenized_csv.csv\"\n",
    "csv_input = \"cleaned_reddit_posts.csv\"\n",
    "df = pd.read_csv(csv_input)\n",
    "\n",
    "# Function to clean the text using regex\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Clean the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokenized_text'] = df['Comments'].apply(word_tokenize)\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if token not in stopwords_english])\n",
    "\n",
    "# Remove tokens with a single character\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if len(token) > 1])\n",
    "\n",
    "# Drop the unnecessary columns (keep only the 'tokenized_text' column)\n",
    "df_cleaned = df[['tokenized_text']]\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_tokenized, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_tokenized}' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### DOWNLOADING AND CACHING MODELS ################################\n",
    "import logging\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define a mapping of languages to model names\n",
    "language_to_model = {\n",
    "    'en': \"IMSyPP/hate_speech_en\",\n",
    "    'it': \"IMSyPP/hate_speech_it\",\n",
    "    'nl': \"IMSyPP/hate_speech_nl\",\n",
    "    'sl': \"IMSyPP/hate_speech_slo\",\n",
    "}\n",
    "\n",
    "# Function to download models to the cache directory\n",
    "def download_models_to_cache():\n",
    "    # Specify the cache directory for local caching\n",
    "    cache_dir = \".cache\"\n",
    "    # Loop over the models and download them to the cache\n",
    "    for model_name in language_to_model.values():\n",
    "        try:\n",
    "            logger.info(f\"Downloading and caching model '{model_name}'...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "            logger.info(f\"Model '{model_name}' downloaded and cached successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while downloading the model '{model_name}': {e}\")\n",
    "\n",
    "# Call the function to download models to the cache\n",
    "download_models_to_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## PROCESSING DATA AND CREATING THE SENTIMENT ANALYSIS ####################\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set the logging level for the transformers library to ERROR\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the Reddit posts CSV file\n",
    "input_csv = \"cleaned_reddit_posts.csv\"\n",
    "output_csv = \"reddit_posts_with_labels.csv\"\n",
    "\n",
    "# Define a mapping of languages to model names\n",
    "language_to_model = {\n",
    "    'en': \"IMSyPP/hate_speech_en\",\n",
    "    'it': \"IMSyPP/hate_speech_it\",\n",
    "    'nl': \"IMSyPP/hate_speech_nl\",\n",
    "    'sl': \"IMSyPP/hate_speech_slo\",\n",
    "}\n",
    "\n",
    "# Define the default model for cases where language detection fails\n",
    "default_model_name = \"IMSyPP/hate_speech_en\"\n",
    "\n",
    "def load_model(language):\n",
    "    model_name = language_to_model.get(language, default_model_name)\n",
    "    cache_dir = \".cache\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    # Set special tokens (if applicable)\n",
    "    special_tokens = {}\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens[\"bos_token\"] = \"[BOS]\"  # Use the string token here\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens[\"eos_token\"] = \"[EOS]\"  # Use the string token here\n",
    "    \n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to analyze a comment and return the results\n",
    "def analyze_comment(comment, language, tokenizer, model):\n",
    "    try:\n",
    "        inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "        analyze_error = None\n",
    "    except Exception as e:\n",
    "        probabilities = [0.0] * 4\n",
    "        analyze_error = str(e)\n",
    "    return probabilities, language, analyze_error\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Total number of comments to process\n",
    "total_comments = df['Comments'].apply(len).sum()\n",
    "\n",
    "# Batch size for writing results to CSV\n",
    "batch_size = 100\n",
    "\n",
    "# Processed comment count\n",
    "processed_comments = 0\n",
    "processed_batch_count = 0\n",
    "\n",
    "# Initialize lists to store final results\n",
    "final_results = []\n",
    "\n",
    "# Load the default model once for cases where language detection fails\n",
    "default_tokenizer, default_model = load_model('en')\n",
    "\n",
    "# Iterate over rows in the CSV\n",
    "progress_bar = tqdm(total=total_comments, desc=\"Processing Comments\", leave=False)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    comment_list = eval(row['Comments'])  # Assuming the comments are in a list format\n",
    "\n",
    "    # Split the comment list into batches\n",
    "    comment_batches = [comment_list[i:i + batch_size] for i in range(0, len(comment_list), batch_size)]\n",
    "\n",
    "    for batch in comment_batches:\n",
    "        try:\n",
    "            # Filter out empty or very short comments\n",
    "            batch = [comment for comment in batch if len(comment.strip()) > 7]  # Adjust the length threshold as needed\n",
    "            \n",
    "            if not batch:\n",
    "                continue  # Skip the batch if no valid comments are present\n",
    "            \n",
    "            # Detect language for the first comment in the batch\n",
    "            language = detect(batch[0])\n",
    "            \n",
    "            # Load the model and tokenizer for the detected language\n",
    "            tokenizer, model = load_model(language)\n",
    "            \n",
    "            # Process the batch of comments sequentially\n",
    "            batch_texts = batch  # Store the batch of comments as a list of strings\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs.logits, dim=1).tolist()\n",
    "            analyze_error = None\n",
    "\n",
    "            # Process the results for each comment in the batch\n",
    "            for i, comment in enumerate(batch_texts):\n",
    "                result_row = (comment,f\"{probabilities[i][0] * 100:.2f}%\",f\"{probabilities[i][1] * 100:.2f}%\",f\"{probabilities[i][2] * 100:.2f}%\",f\"{probabilities[i][3] * 100:.2f}%\",language,analyze_error if analyze_error is not None else \"None\")\n",
    "                final_results.append(result_row)\n",
    "\n",
    "                # Update processed comment count\n",
    "                processed_comments += 1\n",
    "\n",
    "                # Save results to CSV every batch_size comments\n",
    "                if processed_comments % batch_size == 0:\n",
    "                    result_df = pd.DataFrame(final_results, columns=['comment','probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "                    result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "                # Update progress description and postfix\n",
    "                progress_percent = (processed_comments / total_comments) * 100\n",
    "                progress_bar.set_description(f\"Processing Comments - {progress_percent:.2f}%\")\n",
    "                progress_bar.set_postfix({\"Processed\": f\"{processed_comments}/{total_comments}\"})\n",
    "            \n",
    "            # Update progress batch \n",
    "            processed_batch_count += 1\n",
    "            progress_batch = (processed_batch_count / len(comment_batches)) * 100\n",
    "            progress_bar.set_description(f\"Processing Batches - {progress_batch:.2f}%\")\n",
    "            progress_bar.set_postfix({\"Processed\": f\"{processed_batch_count}/{len(comment_batches)}\"})\n",
    "            \n",
    "            result_df = pd.DataFrame(final_results, columns=['comment','probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "            result_df.to_csv(output_csv, index=False, mode='a')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error processing batch: %s\", str(e))\n",
    "\n",
    "# Save the final results\n",
    "result_df = pd.DataFrame(final_results, columns=['comment','probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent', 'language', 'errors'])\n",
    "result_df.to_csv(output_csv, index=False, mode='a')\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "logger.info(\"Analysis completed. Results saved to: %s\", output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"probabilities_acceptable\" at position 1100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\lib.pyx:2280\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"probabilities_acceptable\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Convert percentage strings to numeric values\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mprobabilities_acceptable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mto_numeric(data[\u001b[39m'\u001b[39;49m\u001b[39mprobabilities_acceptable\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mrstrip(\u001b[39m'\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     12\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mprobabilities_hate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mprobabilities_hate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     13\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mprobabilities_offensive\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mprobabilities_offensive\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\tools\\numeric.py:217\u001b[0m, in \u001b[0;36mto_numeric\u001b[1;34m(arg, errors, downcast, dtype_backend)\u001b[0m\n\u001b[0;32m    215\u001b[0m coerce_numeric \u001b[39m=\u001b[39m errors \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m     values, new_mask \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmaybe_convert_numeric(  \u001b[39m# type: ignore[call-overload]  # noqa\u001b[39;49;00m\n\u001b[0;32m    218\u001b[0m         values,\n\u001b[0;32m    219\u001b[0m         \u001b[39mset\u001b[39;49m(),\n\u001b[0;32m    220\u001b[0m         coerce_numeric\u001b[39m=\u001b[39;49mcoerce_numeric,\n\u001b[0;32m    221\u001b[0m         convert_to_masked_nullable\u001b[39m=\u001b[39;49mdtype_backend \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m lib\u001b[39m.\u001b[39;49mno_default\n\u001b[0;32m    222\u001b[0m         \u001b[39mor\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(values_dtype, StringDtype),\n\u001b[0;32m    223\u001b[0m     )\n\u001b[0;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\lib.pyx:2322\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"probabilities_acceptable\" at position 1100"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "csv_path = \"reddit_posts_with_labels.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Clean the data by removing rows with non-numeric values\n",
    "data = data[data['probabilities_acceptable'].str.match(r'^\\d+\\.\\d+%$')]\n",
    "\n",
    "# Convert percentage strings to numeric values\n",
    "data['probabilities_acceptable'] = pd.to_numeric(data['probabilities_acceptable'].str.rstrip('%'))\n",
    "data['probabilities_hate'] = data['probabilities_hate'].str.rstrip('%').astype(float)\n",
    "data['probabilities_offensive'] = data['probabilities_offensive'].str.rstrip('%').astype(float)\n",
    "data['probabilities_violent'] = data['probabilities_violent'].str.rstrip('%').astype(float)\n",
    "\n",
    "# Calculate average probabilities for each content type\n",
    "avg_accept = data['probabilities_acceptable'].mean()\n",
    "avg_hate = data['probabilities_hate'].mean()\n",
    "avg_offensive = data['probabilities_offensive'].mean()\n",
    "avg_violent = data['probabilities_violent'].mean()\n",
    "\n",
    "# Create a bar graph using Matplotlib\n",
    "labels = ['Acceptable', 'Hate', 'Offensive', 'Violent']\n",
    "values = [avg_accept, avg_hate, avg_offensive, avg_violent]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.ylabel('Average Probability')\n",
    "plt.title('Average Probabilities of Content Types')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## GRAPH 2 ####################\n",
    "plt.hist(data['probabilities_acceptable'], bins=20, alpha=0.5, label='Acceptable')\n",
    "plt.hist(data['probabilities_hate'], bins=20, alpha=0.5, label='Hate')\n",
    "plt.hist(data['probabilities_offensive'], bins=20, alpha=0.5, label='Offensive')\n",
    "plt.hist(data['probabilities_violent'], bins=20, alpha=0.5, label='Violent')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Probabilities')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## GRAPH 4 ####################\n",
    "\n",
    "plt.scatter(data['probabilities_acceptable'],data['probabilities_hate'], data['probabilities_offensive'], c=data['probabilities_violent'], cmap='viridis', s=50, alpha=0.7)\n",
    "plt.xlabel('Hate Probability')\n",
    "plt.ylabel('Offensive Probability')\n",
    "plt.title('Scatter Plot of Hate vs. Offensive Probabilities')\n",
    "plt.colorbar(label='Violent Probability')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## GRAPH 5 ####################\n",
    "\n",
    "data[['probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent']].plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Comment')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Content Type Probabilities in Each Comment')\n",
    "plt.legend(title='Content Type')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## GENERATE IMAGES ####################\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "dataset = pd.read_csv(\"reddit_posts_with_labels.csv\", encoding='unicode_escape')\n",
    "\n",
    "# Convert probability columns to numeric (float) type\n",
    "probability_columns = ['probabilities_acceptable', 'probabilities_hate', 'probabilities_offensive', 'probabilities_violent']\n",
    "for column in probability_columns:\n",
    "    dataset[column] = dataset[column].str.rstrip('%').astype(float)\n",
    "\n",
    "# Generate word clouds for each probability column\n",
    "for column in probability_columns:\n",
    "    filtered_comments = ' '.join(\n",
    "        [text for text, prob_value in zip(dataset['comment'], dataset[column]) if prob_value >= 10]) \n",
    "    \n",
    "    # Check if there are words to generate a word cloud\n",
    "    if filtered_comments:\n",
    "        # Generate and display the word cloud\n",
    "        wordcloud = WordCloud(width=800, height=500,\n",
    "                              random_state=21, max_font_size=110).generate(filtered_comments)\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Word Cloud for {column}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No words found for {column}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
