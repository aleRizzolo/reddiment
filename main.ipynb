{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas\n",
    "import praw\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Upvotes\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy',\n",
    "                 'WhitePeopleTwitter', 'politics', 'Republican', 'worldnews']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "    for iteration, submission in enumerate(subreddit.hot(limit=5)):\n",
    "        print(f\"post {iteration}/5\")\n",
    "        # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "        if submission.id not in df[\"Id\"].values:\n",
    "            # Add submission title to the titles list\n",
    "            titles.append(submission.title)\n",
    "            scores.append(submission.score)  # Add upvotes to the scores list\n",
    "            ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "            # Fetch comments for the current submission\n",
    "            submission.comments.replace_more(limit=25)\n",
    "            submission_comments = []\n",
    "            for comment in submission.comments.list():\n",
    "                # Check if the comment author's username contains \"bot\"\n",
    "                if 'bot' not in comment.name:\n",
    "                    # Use BeautifulSoup to remove HTML tags from content\n",
    "                    soup = BeautifulSoup(comment.body, 'lxml')\n",
    "                    filtered_content = soup.get_text()\n",
    "\n",
    "                    # Remove URLs from filtered_content\n",
    "                    filtered_content = re.sub(\n",
    "                        r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                    # Remove only #\n",
    "                    filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                    submission_comments.append(filtered_content)\n",
    "            comments.append(submission_comments)\n",
    "\n",
    "        # Create a DataFrame with the new data\n",
    "        new_data = pandas.DataFrame(\n",
    "            {\"Title\": titles, \"Id\": ids, \"Upvotes\": scores, \"Comments\": comments}\n",
    "        )\n",
    "\n",
    "        # Append/concat the new data to the existing DataFrame\n",
    "        df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "        df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "    # Save the DataFrame to the CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "csv_tokenized = \"tokenized_csv.csv\"\n",
    "csv_input = \"reddit_posts_with_comments.csv\"\n",
    "df = pd.read_csv(csv_input)\n",
    "\n",
    "# Function to clean the text using regex\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# Clean the 'Comments' column\n",
    "df['Comments'] = df['Comments'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokenized_text'] = df['Comments'].apply(word_tokenize)\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if token not in stopwords_english])\n",
    "\n",
    "# Remove tokens with a single character\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda tokens: [token for token in tokens if len(token) > 1])\n",
    "\n",
    "# Drop the unnecessary columns (keep only the 'tokenized_text' column)\n",
    "df_cleaned = df[['tokenized_text']]\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_tokenized, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_tokenized}' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the Reddit posts CSV file\n",
    "input_csv = \"reddit_posts_with_comments.csv\"\n",
    "output_csv = \"reddit_posts_with_labels.csv\"\n",
    "\n",
    "# Define a mapping of languages to model names\n",
    "language_to_model = {\n",
    "    'en': \"IMSyPP/hate_speech_en\",\n",
    "    'it': \"IMSyPP/hate_speech_it\",\n",
    "    'nl': \"IMSyPP/hate_speech_nl\",\n",
    "    'sl': \"IMSyPP/hate_speech_slo\",\n",
    "}\n",
    "\n",
    "# Define the default model for cases where language detection fails\n",
    "default_model_name = \"IMSyPP/hate_speech_en\"\n",
    "\n",
    "# Function to load the appropriate model based on the language\n",
    "def load_model(language):\n",
    "    model_name = language_to_model.get(language, default_model_name)\n",
    "\n",
    "    # Specify the cache directory for local caching\n",
    "    cache_dir = \".cache\"\n",
    "\n",
    "    # Use a retry mechanism with a timeout\n",
    "    retries = 3\n",
    "    timeout = 30  # seconds\n",
    "\n",
    "    # Retry mechanism to handle timeout issues\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            # Load the tokenizer and model\n",
    "            logger.info(f\"Downloading and caching model '{model_name}'...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, timeout=timeout)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir, timeout=timeout)\n",
    "            logger.info(f\"Model '{model_name}' downloaded and cached successfully.\")\n",
    "            return tokenizer, model\n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.warning(\"Request timed out, retrying model download...\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while downloading the model: {e}\")\n",
    "            break\n",
    "    else:\n",
    "        logger.error(f\"Failed to load model '{model_name}' after {retries} retries.\")\n",
    "        return None, None\n",
    "\n",
    "# Function to analyze a comment and return the results\n",
    "def analyze_comment(comment, language, tokenizer, model):\n",
    "    # Tokenize the comment\n",
    "    inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the probabilities for each class (non-hate speech, hate speech, offensive speech, violent speech)\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "\n",
    "    return probabilities, language\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Total number of comments to process\n",
    "total_comments = df['Comments'].apply(len).sum()\n",
    "\n",
    "# Batch size for writing results to CSV\n",
    "batch_size = 100\n",
    "\n",
    "# Function to process a batch of comments and store the results in the result lists\n",
    "def process_batch(comment_list, tokenizer, model):\n",
    "    result_comments = []\n",
    "    result_probabilities_hate = []\n",
    "    result_probabilities_offensive = []\n",
    "    result_probabilities_violent = []\n",
    "    result_languages = []\n",
    "\n",
    "    for comment in comment_list:\n",
    "        try:\n",
    "            # Detect the language of the comment\n",
    "            language = detect(comment)\n",
    "        except:\n",
    "            # Handle language detection errors by using the default model\n",
    "            language = 'en'\n",
    "\n",
    "        # Analyze the comment using the appropriate model\n",
    "        probabilities, language = analyze_comment(comment, language, tokenizer, model)\n",
    "\n",
    "        # Store the results\n",
    "        result_comments.append(comment)\n",
    "        result_probabilities_hate.append(probabilities[1])\n",
    "        result_probabilities_offensive.append(probabilities[2])\n",
    "        result_probabilities_violent.append(probabilities[3])\n",
    "        result_languages.append(language)\n",
    "\n",
    "    return result_comments, result_probabilities_hate, result_probabilities_offensive, result_probabilities_violent, result_languages\n",
    "\n",
    "# Initialize lists to store final results\n",
    "final_result_comments = []\n",
    "final_result_probabilities_hate = []\n",
    "final_result_probabilities_offensive = []\n",
    "final_result_probabilities_violent = []\n",
    "final_result_languages = []\n",
    "\n",
    "# Processed comment count\n",
    "processed_comments = 0\n",
    "\n",
    "# Iterate over rows in the CSV\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Comments\"):\n",
    "    comment_list = row['Comments']\n",
    "\n",
    "    # Split the comment list into batches\n",
    "    comment_batches = [comment_list[i:i + batch_size] for i in range(0, len(comment_list), batch_size)]\n",
    "\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer, model = load_model('en')  # Default model\n",
    "\n",
    "    # Process batches concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(process_batch, batch, tokenizer, model) for batch in comment_batches]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                # Get the results from each batch\n",
    "                result_comments, result_probabilities_hate, result_probabilities_offensive, result_probabilities_violent, result_languages = future.result()\n",
    "\n",
    "                # Append the results to the final lists\n",
    "                final_result_comments.extend(result_comments)\n",
    "                final_result_probabilities_hate.extend(result_probabilities_hate)\n",
    "                final_result_probabilities_offensive.extend(result_probabilities_offensive)\n",
    "                final_result_probabilities_violent.extend(result_probabilities_violent)\n",
    "                final_result_languages.extend(result_languages)\n",
    "\n",
    "                # Update processed comment count\n",
    "                processed_comments += len(result_comments)\n",
    "\n",
    "                # Log progress\n",
    "                progress_percent = (processed_comments / total_comments) * 100\n",
    "                logger.info(f\"Processed {processed_comments}/{total_comments} comments ({progress_percent:.2f}%)\")\n",
    "            except Exception as e:\n",
    "                # Log errors and continue processing other batches\n",
    "                logger.error(f\"An error occurred while processing a batch: {e}\")\n",
    "\n",
    "# Create a new DataFrame for the final results\n",
    "result_df = pd.DataFrame({\n",
    "    'comment': final_result_comments,\n",
    "    'probabilities_hate': final_result_probabilities_hate,\n",
    "    'probabilities_offensive': final_result_probabilities_offensive,\n",
    "    'probabilities_violent': final_result_probabilities_violent,\n",
    "    'language': final_result_languages\n",
    "})\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "logger.info(\"Analysis completed. Results saved to: %s\", output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
