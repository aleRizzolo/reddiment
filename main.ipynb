{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas\n",
    "import praw\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "# Authenticate with Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=env[\"CLIENT_ID\"],\n",
    "    client_secret=env[\"CLIENT_SECRET\"],\n",
    "    user_agent=env[\"USER_AGENT\"],\n",
    "    redirect_uri=env[\"REDIRECT_URI\"],\n",
    "    refresh_token=env[\"REFRESH_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "csv_file_name = \"reddit_posts_with_comments.csv\"\n",
    "if os.path.exists(csv_file_name):\n",
    "    print(\"CSV file already exists. Appending new data and avoiding duplicates.\")\n",
    "    df = pandas.read_csv(csv_file_name)  # Read existing CSV into a DataFrame\n",
    "else:\n",
    "    print(\"CSV file does not exist. It will be created after fetching new data.\")\n",
    "    df = pandas.DataFrame(columns=[\"Title\", \"Id\", \"Upvotes\", \"Comments\"])\n",
    "\n",
    "# Create a subreddit instance\n",
    "targetObjects = ['conspiracy',\n",
    "                 'WhitePeopleTwitter', 'politics', 'Republican', 'worldnews']\n",
    "for subreddit_name in targetObjects:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Print subreddit name\n",
    "    print(subreddit.display_name)\n",
    "\n",
    "    # Lists to store submission information\n",
    "    titles = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    comments = []\n",
    "\n",
    "    # Loop through the newest 21 submissions in the subreddit\n",
    "    for iteration, submission in enumerate(subreddit.hot(limit=5)):\n",
    "        print(f\"post {iteration}/5\")\n",
    "        # Check if the submission ID already exists in the DataFrame to avoid duplication\n",
    "        if submission.id not in df[\"Id\"].values:\n",
    "            # Add submission title to the titles list\n",
    "            titles.append(submission.title)\n",
    "            scores.append(submission.score)  # Add upvotes to the scores list\n",
    "            ids.append(submission.id)  # Add submission ID to the ids list\n",
    "\n",
    "            # Fetch comments for the current submission\n",
    "            submission.comments.replace_more(limit=25)\n",
    "            submission_comments = []\n",
    "            for comment in submission.comments.list():\n",
    "                # Check if the comment author's username contains \"bot\"\n",
    "                if 'bot' not in comment.name:\n",
    "                    # Use BeautifulSoup to remove HTML tags from content\n",
    "                    soup = BeautifulSoup(comment.body, 'lxml')\n",
    "                    filtered_content = soup.get_text()\n",
    "\n",
    "                    # Remove URLs from filtered_content\n",
    "                    filtered_content = re.sub(\n",
    "                        r'http\\S+|www\\S+', '', filtered_content)\n",
    "\n",
    "                    # Remove only #\n",
    "                    filtered_content = re.sub(r'#', '', filtered_content).lower()\n",
    "                    submission_comments.append(filtered_content)\n",
    "            comments.append(submission_comments)\n",
    "\n",
    "        # Create a DataFrame with the new data\n",
    "        new_data = pandas.DataFrame(\n",
    "            {\"Title\": titles, \"Id\": ids, \"Upvotes\": scores, \"Comments\": comments}\n",
    "        )\n",
    "\n",
    "        # Append/concat the new data to the existing DataFrame\n",
    "        df = pandas.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on the 'Id' column (submission IDs)\n",
    "        df.drop_duplicates(subset=\"Id\", keep=\"last\", inplace=True)\n",
    "    # Save the DataFrame to the CSV file\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"CSV file '{csv_file_name}' has been generated/updated with the new Reddit posts and comments while avoiding duplicates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4)\n",
      "                                               Title     Id  \\\n",
      "0  A secret Chinese biolab was just uncovered in ...  elz g   \n",
      "1                              ADHD  is a conspiracy  eg bt   \n",
      "2  A Shadow Banned Post  potentially the biggest ...  egrlo   \n",
      "3  Yuval Harari  Conspiracy Theorists must be eli...  e zdc   \n",
      "4  The ancient Sumerians tell a story that might ...  eo su   \n",
      "5      Joe Rogan never failing to show his true side  eixqd   \n",
      "6  Whens the last time anyone saw Mar Lard o at t...    eed   \n",
      "7                         Compensating for something  eeyxl   \n",
      "8  That s the difference between a regular politi...  ej np   \n",
      "9            Heck of a legal argument he s got there  efl m   \n",
      "\n",
      "                                            Comments  \\\n",
      "0  meta  sticky comment n n rule        does not ...   \n",
      "1  meta  sticky comment n n rule        does not ...   \n",
      "2  meta  sticky comment n n rule        does not ...   \n",
      "3  meta  sticky comment n n rule        does not ...   \n",
      "4  meta  sticky comment n n rule        does not ...   \n",
      "5  trump was still in office for weeks afterwards...   \n",
      "6  the ghost writer of his book heavily implied t...   \n",
      "7  redneck rich  they got more value in their gun...   \n",
      "8  if that were trump  he would have sold tickets...   \n",
      "9  he has no problem bringing up hillarys emails ...   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [meta, sticky, comment, n, n, rule, apply, rep...  \n",
      "1  [meta, sticky, comment, n, n, rule, apply, rep...  \n",
      "2  [meta, sticky, comment, n, n, rule, apply, rep...  \n",
      "3  [meta, sticky, comment, n, n, rule, apply, rep...  \n",
      "4  [meta, sticky, comment, n, n, rule, apply, rep...  \n",
      "5  [trump, still, office, weeks, afterwards, n, n...  \n",
      "6  [ghost, writer, book, heavily, implied, trump,...  \n",
      "7  [redneck, rich, got, value, guns, home, spends...  \n",
      "8  [trump, would, sold, tickets, members, crowd, ...  \n",
      "9  [problem, bringing, hillarys, emails, hunter, ...  \n",
      "CSV file 'tokenized_csv.csv' has been generated/updated with the new Reddit posts and comments while avoiding duplicates and cleaning the text data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "csv_tokenized = \"tokenized_csv.csv\"\n",
    "csv_input = \"reddit_posts_with_comments.csv\"\n",
    "df = pd.read_csv(csv_input)\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned = df_cleaned.select_dtypes(include=['object']).applymap(\n",
    "    lambda x: re.sub(r'[^a-zA-Z]', ' ', str(x)))\n",
    "df_cleaned = df_cleaned.applymap(\n",
    "    lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df_cleaned = df_cleaned.replace('nan', np.nan).dropna()\n",
    "\n",
    "# Tokenize the text data\n",
    "df_cleaned['tokenized_text'] = df_cleaned['Comments'].apply(\n",
    "    lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Removal of stopwords\n",
    "stopwords_english = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords_italian = set(nltk.corpus.stopwords.words(\"italian\"))\n",
    "df_cleaned['tokenized_text'] = df_cleaned['tokenized_text'].apply(lambda tokens: [\n",
    "    token for token in tokens if token.lower() not in stopwords_english and token.lower() not in stopwords_italian])\n",
    "\n",
    "# Drop the unnecessary columns (keep only the 'tokenized_text' column)\n",
    "df_cleaned = df_cleaned[['tokenized_text']]\n",
    "\n",
    "# Save the cleaned DataFrame to the CSV file\n",
    "df_cleaned.to_csv(csv_tokenized, index=False)\n",
    "\n",
    "# Print the shape of the DataFrame and display the first 10 rows\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.head(10))\n",
    "\n",
    "print(\n",
    "    f\"CSV file '{csv_tokenized}' has been generated/updated with the tokenized text while avoiding duplicates and cleaning the data.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
